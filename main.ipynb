{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama_index\n",
      "  Downloading llama_index-0.12.10-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama_index)\n",
      "  Downloading llama_index_agent_openai-0.4.1-py3-none-any.whl.metadata (726 bytes)\n",
      "Collecting llama-index-cli<0.5.0,>=0.4.0 (from llama_index)\n",
      "  Downloading llama_index_cli-0.4.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-core<0.13.0,>=0.12.10 (from llama_index)\n",
      "  Downloading llama_index_core-0.12.10.post1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama_index)\n",
      "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama_index)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.6.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama_index)\n",
      "  Downloading llama_index_llms_openai-0.3.13-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 (from llama_index)\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.4.2-py3-none-any.whl.metadata (726 bytes)\n",
      "Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama_index)\n",
      "  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama_index)\n",
      "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
      "Collecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama_index)\n",
      "  Downloading llama_index_readers_file-0.4.3-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama_index)\n",
      "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from llama_index) (3.9.1)\n",
      "Requirement already satisfied: openai>=1.14.0 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama_index) (1.57.3)\n",
      "Collecting PyYAML>=6.0.1 (from llama-index-core<0.13.0,>=0.12.10->llama_index)\n",
      "  Downloading PyYAML-6.0.2-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.10->llama_index) (2.0.35)\n",
      "Collecting aiohttp<4.0.0,>=3.8.6 (from llama-index-core<0.13.0,>=0.12.10->llama_index)\n",
      "  Downloading aiohttp-3.11.11-cp311-cp311-win_amd64.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama_index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama_index) (1.2.15)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.10->llama_index)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama_index) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama_index) (2023.6.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama_index) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama_index) (1.6.0)\n",
      "Collecting networkx>=3.0 (from llama-index-core<0.13.0,>=0.12.10->llama_index)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama_index) (1.25.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama_index) (10.0.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama_index) (2.9.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama_index) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama_index) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama_index) (0.8.0)\n",
      "Collecting tqdm<5.0.0,>=4.66.1 (from llama-index-core<0.13.0,>=0.12.10->llama_index)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama_index) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama_index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama_index) (1.17.0)\n",
      "Collecting llama-cloud>=0.1.5 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama_index)\n",
      "  Downloading llama_cloud-0.1.8-py3-none-any.whl.metadata (860 bytes)\n",
      "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.5.0,>=0.4.0->llama_index)\n",
      "  Downloading openai-1.59.6-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama_index) (4.12.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama_index) (2.0.3)\n",
      "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama_index) (5.1.0)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5.0,>=0.4.0->llama_index)\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_parse-0.5.19-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: click in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk>3.8.1->llama_index) (8.1.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk>3.8.1->llama_index) (1.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk>3.8.1->llama_index) (2024.11.6)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.10->llama_index)\n",
      "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.10->llama_index) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.10->llama_index) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.10->llama_index) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.10->llama_index) (6.0.4)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.10->llama_index)\n",
      "  Downloading propcache-0.2.1-cp311-cp311-win_amd64.whl.metadata (9.5 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.10->llama_index)\n",
      "  Downloading yarl-1.18.3-cp311-cp311-win_amd64.whl.metadata (71 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama_index) (2.6)\n",
      "Collecting certifi<2025.0.0,>=2024.7.4 (from llama-cloud>=0.1.5->llama-index-indices-managed-llama-cloud>=0.4.0->llama_index)\n",
      "  Downloading certifi-2024.12.14-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.10->llama_index) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.10->llama_index) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.10->llama_index) (3.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.10->llama_index) (0.14.0)\n",
      "Collecting click (from nltk>3.8.1->llama_index)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->nltk>3.8.1->llama_index) (0.4.6)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama_index) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama_index) (0.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama_index) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.10->llama_index) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.10->llama_index) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.10->llama_index) (3.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.10->llama_index) (2.0.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.10->llama_index) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.10->llama_index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.10->llama_index) (3.23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama_index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama_index) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama_index) (2023.3)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.10->llama_index) (24.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\darren\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama_index) (1.16.0)\n",
      "Downloading llama_index-0.12.10-py3-none-any.whl (6.8 kB)\n",
      "Downloading llama_index_agent_openai-0.4.1-py3-none-any.whl (13 kB)\n",
      "Downloading llama_index_cli-0.4.0-py3-none-any.whl (27 kB)\n",
      "Downloading llama_index_core-0.12.10.post1-py3-none-any.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.6/1.6 MB 16.9 MB/s eta 0:00:00\n",
      "Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.6.3-py3-none-any.whl (11 kB)\n",
      "Downloading llama_index_llms_openai-0.3.13-py3-none-any.whl (14 kB)\n",
      "Downloading llama_index_multi_modal_llms_openai-0.4.2-py3-none-any.whl (5.9 kB)\n",
      "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
      "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
      "Downloading llama_index_readers_file-0.4.3-py3-none-any.whl (38 kB)\n",
      "Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
      "Downloading aiohttp-3.11.11-cp311-cp311-win_amd64.whl (442 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading llama_cloud-0.1.8-py3-none-any.whl (247 kB)\n",
      "Downloading llama_parse-0.5.19-py3-none-any.whl (15 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 23.6 MB/s eta 0:00:00\n",
      "Downloading openai-1.59.6-py3-none-any.whl (454 kB)\n",
      "Downloading PyYAML-6.0.2-cp311-cp311-win_amd64.whl (161 kB)\n",
      "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Downloading certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
      "Downloading propcache-0.2.1-cp311-cp311-win_amd64.whl (44 kB)\n",
      "Downloading yarl-1.18.3-cp311-cp311-win_amd64.whl (91 kB)\n",
      "Installing collected packages: striprtf, dirtyjson, tqdm, PyYAML, propcache, networkx, click, certifi, aiohappyeyeballs, yarl, aiohttp, openai, llama-index-core, llama-cloud, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama_index\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.65.0\n",
      "    Uninstalling tqdm-4.65.0:\n",
      "      Successfully uninstalled tqdm-4.65.0\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n",
      "      Successfully uninstalled PyYAML-6.0\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.1.5\n",
      "    Uninstalling click-8.1.5:\n",
      "      Successfully uninstalled click-8.1.5\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2023.5.7\n",
      "    Uninstalling certifi-2023.5.7:\n",
      "      Successfully uninstalled certifi-2023.5.7\n",
      "  Attempting uninstall: yarl\n",
      "    Found existing installation: yarl 1.9.2\n",
      "    Uninstalling yarl-1.9.2:\n",
      "      Successfully uninstalled yarl-1.9.2\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.8.4\n",
      "    Uninstalling aiohttp-3.8.4:\n",
      "      Successfully uninstalled aiohttp-3.8.4\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.57.3\n",
      "    Uninstalling openai-1.57.3:\n",
      "      Successfully uninstalled openai-1.57.3\n",
      "Successfully installed PyYAML-6.0.2 aiohappyeyeballs-2.4.4 aiohttp-3.11.11 certifi-2024.12.14 click-8.1.8 dirtyjson-1.0.8 llama-cloud-0.1.8 llama-index-agent-openai-0.4.1 llama-index-cli-0.4.0 llama-index-core-0.12.10.post1 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.3 llama-index-llms-openai-0.3.13 llama-index-multi-modal-llms-openai-0.4.2 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.3 llama-index-readers-llama-parse-0.4.0 llama-parse-0.5.19 llama_index-0.12.10 networkx-3.4.2 openai-1.59.6 propcache-0.2.1 striprtf-0.0.26 tqdm-4.67.1 yarl-1.18.3\n"
     ]
    }
   ],
   "source": [
    "!pip install llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from llama_index.core import SimpleDirectoryReader, Settings, SummaryIndex, VectorStoreIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker, AgentRunner\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Set API key from environment variable\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Check if the API key is set\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable is not set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define papers\n",
    "papers = [\n",
    "    \"paper_1.pdf\",\n",
    "    \"paper_2.pdf\",\n",
    "    \"paper_3.pdf\",\n",
    "    \"paper_4.pdf\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up LLM and embedding model\n",
    "Settings.llm = OpenAI(model=\"gpt-4o\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get document tools\n",
    "def get_doc_tools(paper, paper_name):\n",
    "    documents = SimpleDirectoryReader(input_files=[paper]).load_data()\n",
    "    splitter = SentenceSplitter(chunk_size=1024)\n",
    "    nodes = splitter.get_nodes_from_documents(documents)\n",
    "    \n",
    "    summary_index = SummaryIndex(nodes)\n",
    "    vector_index = VectorStoreIndex(nodes)\n",
    "    \n",
    "    summary_query_engine = summary_index.as_query_engine(\n",
    "        response_mode=\"tree_summarize\",\n",
    "        use_async=True,\n",
    "    )\n",
    "    vector_query_engine = vector_index.as_query_engine()\n",
    "    \n",
    "    summary_tool = QueryEngineTool.from_defaults(\n",
    "        query_engine=summary_query_engine,\n",
    "        description=f\"Useful for summarization questions related to {paper_name}\"\n",
    "    )\n",
    "    vector_tool = QueryEngineTool.from_defaults(\n",
    "        query_engine=vector_query_engine,\n",
    "        description=f\"Useful for retrieving specific context from {paper_name}\"\n",
    "    )\n",
    "    \n",
    "    return vector_tool, summary_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: paper_1.pdf\n",
      "Getting tools for paper: paper_2.pdf\n",
      "Getting tools for paper: paper_3.pdf\n",
      "Getting tools for paper: paper_4.pdf\n"
     ]
    }
   ],
   "source": [
    "# Create tools for each paper\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all tools\n",
    "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create object index for tool retrieval\n",
    "obj_index = ObjectIndex.from_objects(all_tools, index_cls=VectorStoreIndex)\n",
    "obj_retriever = obj_index.as_retriever(similarity_top_k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent\n",
    "llm = OpenAI(model=\"gpt-4o\")\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tool_retriever=obj_retriever,\n",
    "    llm=llm, \n",
    "    system_prompt=\"\"\" \\\n",
    "You are an agent designed to answer queries about TSTR methodology and synthetic data quality testing.\n",
    "Please always use the tools provided to answer questions about TSTR and synthetic data evaluation. Do not rely on prior knowledge.\\\n",
    "\"\"\",\n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Explain the key components of the TSTR methodology for evaluating synthetic data quality.\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"Explain the key components of the TSTR methodology for evaluating synthetic data quality.\"}\n",
      "=== Function Output ===\n",
      "The TSTR (Train on Synthetic, Test on Real) methodology for evaluating synthetic data quality involves training a machine learning model on synthetic data and then testing its performance on real data. This approach assesses how well the synthetic data can replicate the patterns and relationships present in the real data. The key components of TSTR include:\n",
      "\n",
      "1. **Training Phase**: A model is trained using the synthetic dataset. This phase focuses on capturing the underlying patterns and relationships that the synthetic data is meant to represent.\n",
      "\n",
      "2. **Testing Phase**: The trained model is then evaluated using a real dataset. The performance metrics obtained during this phase, such as accuracy, AUROC, or AUPRC, indicate how well the model generalizes to real-world data.\n",
      "\n",
      "3. **Comparison of Results**: The performance of the model trained on synthetic data is compared to a model trained and tested on real data. A close match in performance metrics suggests that the synthetic data has high utility and can effectively mimic the real data.\n",
      "\n",
      "4. **Utility Assessment**: The methodology provides insights into the utility of synthetic data by determining if it can be used as a proxy for real data in predictive modeling tasks.\n",
      "\n",
      "Overall, TSTR is a practical approach to evaluate whether synthetic data can be reliably used for analysis and decision-making in place of real data.\n",
      "=== LLM Response ===\n",
      "The TSTR (Train on Synthetic, Test on Real) methodology for evaluating synthetic data quality involves several key components:\n",
      "\n",
      "1. **Training Phase**: A machine learning model is trained using the synthetic dataset. This phase aims to capture the patterns and relationships that the synthetic data is designed to represent.\n",
      "\n",
      "2. **Testing Phase**: The trained model is evaluated using a real dataset. Performance metrics such as accuracy, AUROC, or AUPRC are used to assess how well the model generalizes to real-world data.\n",
      "\n",
      "3. **Comparison of Results**: The performance of the model trained on synthetic data is compared to a model trained and tested on real data. A close match in performance metrics indicates that the synthetic data has high utility and effectively mimics the real data.\n",
      "\n",
      "4. **Utility Assessment**: This methodology provides insights into the utility of synthetic data by determining if it can be used as a proxy for real data in predictive modeling tasks.\n",
      "\n",
      "Overall, TSTR evaluates whether synthetic data can be reliably used for analysis and decision-making in place of real data.\n",
      "The TSTR (Train on Synthetic, Test on Real) methodology for evaluating synthetic data quality involves several key components:\n",
      "\n",
      "1. **Training Phase**: A machine learning model is trained using the synthetic dataset. This phase aims to capture the patterns and relationships that the synthetic data is designed to represent.\n",
      "\n",
      "2. **Testing Phase**: The trained model is evaluated using a real dataset. Performance metrics such as accuracy, AUROC, or AUPRC are used to assess how well the model generalizes to real-world data.\n",
      "\n",
      "3. **Comparison of Results**: The performance of the model trained on synthetic data is compared to a model trained and tested on real data. A close match in performance metrics indicates that the synthetic data has high utility and effectively mimics the real data.\n",
      "\n",
      "4. **Utility Assessment**: This methodology provides insights into the utility of synthetic data by determining if it can be used as a proxy for real data in predictive modeling tasks.\n",
      "\n",
      "Overall, TSTR evaluates whether synthetic data can be reliably used for analysis and decision-making in place of real data.\n",
      "Added user message to memory: Compare and contrast the approaches used in different papers for testing synthetic data quality using TSTR. Analyze the methodology in each paper first.\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"Analyze the methodology for testing synthetic data quality using TSTR in paper 1.\"}\n",
      "=== Function Output ===\n",
      "The methodology for testing synthetic data quality using Train on Synthetic, Test on Real (TSTR) involves evaluating the utility of synthetic data by comparing the performance of models trained on synthetic data against those trained on real data. In the context of the study, a gradient boosted classification tree model was developed to predict COVID-19 mortality using both real and synthetic datasets. The model's accuracy was assessed using metrics such as the Area Under the Receiver Operating Characteristic curve (AUROC) and the Area Under the Precision-Recall Curve (AUPRC). The results from the synthetic data were then compared to those from the real data to determine the extent to which the synthetic data could replicate the patterns and conclusions derived from the real data. This approach helps in assessing whether synthetic data can serve as a reliable proxy for real data in predictive modeling tasks.\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"Analyze the methodology for testing synthetic data quality using TSTR in paper 2.\"}\n",
      "=== Function Output ===\n",
      "The provided context does not contain information about the methodology for testing synthetic data quality using TSTR (Train on Synthetic, Test on Real) in paper 2. Therefore, I cannot provide an analysis of that methodology based on the given information.\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"Analyze the methodology for testing synthetic data quality using TSTR in paper 3.\"}\n",
      "=== Function Output ===\n",
      "The provided context does not contain information about the methodology for testing synthetic data quality using TSTR (Train on Synthetic, Test on Real) in paper 3. Therefore, I am unable to analyze the methodology for TSTR based on the given information.\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"Analyze the methodology for testing synthetic data quality using TSTR in paper 4.\"}\n",
      "=== Function Output ===\n",
      "The methodology for testing synthetic data quality in the paper involves comparing the analysis results between real and synthetic datasets. Specifically, the study constructs a prediction model of COVID-19 mortality using real data and compares it to the same model developed on synthetic data. The primary analysis uses a machine learning technique, specifically gradient boosted classification trees, to predict death. The model's accuracy is assessed using metrics such as the Area Under the Receiver Operating Characteristic curve (AUROC) and the Area Under the Precision-Recall Curve (AUPRC). The study evaluates the utility of synthetic data by examining the extent to which it can replicate the accuracy and functional relationships of the model developed on real data. Additionally, the study performs a distinguishability test to assess whether a classifier can differentiate between real and synthetic datasets, and evaluates privacy risks associated with the synthetic data.\n",
      "=== LLM Response ===\n",
      "Here's a comparison of the methodologies used in different papers for testing synthetic data quality using the Train on Synthetic, Test on Real (TSTR) approach:\n",
      "\n",
      "1. **Paper 1:**\n",
      "   - **Methodology:** The study uses a gradient boosted classification tree model to predict COVID-19 mortality. The model is trained on both real and synthetic datasets. The performance is evaluated using metrics like AUROC and AUPRC. The results from synthetic data are compared to those from real data to assess the synthetic data's ability to replicate patterns and conclusions from real data.\n",
      "\n",
      "2. **Paper 2:**\n",
      "   - **Methodology:** Information about the methodology for testing synthetic data quality using TSTR in this paper is not available.\n",
      "\n",
      "3. **Paper 3:**\n",
      "   - **Methodology:** Information about the methodology for testing synthetic data quality using TSTR in this paper is not available.\n",
      "\n",
      "4. **Paper 4:**\n",
      "   - **Methodology:** Similar to Paper 1, this study constructs a prediction model for COVID-19 mortality using gradient boosted classification trees. The model's accuracy is evaluated using AUROC and AUPRC. The study compares the model's performance on real and synthetic data to assess the synthetic data's utility. Additionally, it includes a distinguishability test to see if a classifier can differentiate between real and synthetic datasets and evaluates privacy risks.\n",
      "\n",
      "**Comparison:**\n",
      "- Both Paper 1 and Paper 4 use a similar approach involving gradient boosted classification trees and evaluate model performance using AUROC and AUPRC. They focus on comparing the predictive accuracy of models trained on synthetic versus real data.\n",
      "- Paper 4 adds an extra layer of analysis by performing a distinguishability test and assessing privacy risks, which is not mentioned in Paper 1.\n",
      "- Papers 2 and 3 lack available information on their methodologies for TSTR, so no comparison can be made for these papers.\n",
      "Here's a comparison of the methodologies used in different papers for testing synthetic data quality using the Train on Synthetic, Test on Real (TSTR) approach:\n",
      "\n",
      "1. **Paper 1:**\n",
      "   - **Methodology:** The study uses a gradient boosted classification tree model to predict COVID-19 mortality. The model is trained on both real and synthetic datasets. The performance is evaluated using metrics like AUROC and AUPRC. The results from synthetic data are compared to those from real data to assess the synthetic data's ability to replicate patterns and conclusions from real data.\n",
      "\n",
      "2. **Paper 2:**\n",
      "   - **Methodology:** Information about the methodology for testing synthetic data quality using TSTR in this paper is not available.\n",
      "\n",
      "3. **Paper 3:**\n",
      "   - **Methodology:** Information about the methodology for testing synthetic data quality using TSTR in this paper is not available.\n",
      "\n",
      "4. **Paper 4:**\n",
      "   - **Methodology:** Similar to Paper 1, this study constructs a prediction model for COVID-19 mortality using gradient boosted classification trees. The model's accuracy is evaluated using AUROC and AUPRC. The study compares the model's performance on real and synthetic data to assess the synthetic data's utility. Additionally, it includes a distinguishability test to see if a classifier can differentiate between real and synthetic datasets and evaluates privacy risks.\n",
      "\n",
      "**Comparison:**\n",
      "- Both Paper 1 and Paper 4 use a similar approach involving gradient boosted classification trees and evaluate model performance using AUROC and AUPRC. They focus on comparing the predictive accuracy of models trained on synthetic versus real data.\n",
      "- Paper 4 adds an extra layer of analysis by performing a distinguishability test and assessing privacy risks, which is not mentioned in Paper 1.\n",
      "- Papers 2 and 3 lack available information on their methodologies for TSTR, so no comparison can be made for these papers.\n"
     ]
    }
   ],
   "source": [
    "response1 = agent.query(\n",
    "    \"Explain the key components of the TSTR methodology for evaluating synthetic data quality.\"\n",
    ")\n",
    "print(str(response1))\n",
    "\n",
    "response2 = agent.query(\n",
    "    \"Compare and contrast the approaches used in different papers for testing synthetic data quality using TSTR. \"\n",
    "    \"Analyze the methodology in each paper first.\"\n",
    ")\n",
    "print(str(response2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
